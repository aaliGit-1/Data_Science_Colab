{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGbvlOCrr32cubRcKFNeH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaliGit-1/Data_Science_Colab/blob/main/DataPrePro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbfhk2QBUXyR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data_set=pd.read_csv(\"USRegionalMortalityNew.csv\")\n",
        "#data_set.head(20)\n",
        "#del_columns=[\"Rate\",\"SE\"]\n",
        "#data_set.drop(columns=[\"rownames\",\"Rate\",\"SE\"],inplace=True)\n",
        "#data_set.head(10)\n",
        "'''n = len(pd.unique(data_set['Cause']))\n",
        "print(\"Different causes: \",n)#output is 11 as it counts NaN also\n",
        "m = len(pd.unique(data_set['Region']))\n",
        "print(\"Different region: \",m)#output is 11 as it counts NaN also\n",
        "data_set'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Searching for missing values in the columns\n",
        "import numpy as np\n",
        "missing_df=data_set.isnull()\n",
        "#missing_df\n",
        "'''\n",
        "for cols in data_set.columns:\n",
        "  print('ColumnName:',cols)\n",
        "  print(missing_df[cols].value_counts())\n",
        "  print(\" \")\n",
        "'''\n",
        "data_set.head(10)"
      ],
      "metadata": {
        "id": "Z2mMu5UDXcQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handling the missing values\n",
        "from sklearn.impute import SimpleImputer\n",
        "object_col=data_set.select_dtypes(include='object')\n",
        "imputer_mf=SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
        "data_set[object_col.columns]=imputer_mf.fit_transform(data_set[object_col.columns])\n",
        "data_set[object_col.columns].head(20)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Jp29G3VVcDTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14b4b16-f5f8-4ca8-f0b2-3995d1f1b0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Different causes:  10\n",
            "Different region:  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding Categorical Data using One-Hot Encoding Technique\n",
        "#Cause\n",
        "cause_dummy=pd.get_dummies(data_set['Cause'],dtype=int)\n",
        "data_set=pd.concat([data_set,cause_dummy],axis=1)\n",
        "data_set.drop('Cause',axis=1,inplace=True)\n",
        "#Region\n",
        "region_dummy=pd.get_dummies(data_set['Region'],dtype=int)\n",
        "data_set=pd.concat([data_set,region_dummy],axis=1)\n",
        "data_set.drop('Region',axis=1,inplace=True)\n",
        "data_set.head(10)"
      ],
      "metadata": {
        "id": "CXn4nAOJgA5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using One-Hot Encoding Technique to encode categorical data which contains more than two unique values in the column.<br>\n",
        ">//line1 and line4: Using pandas function 'get_dummies' to create columns representing the unique values present in the 'Cause' and 'Region' columns and storing it to 'cause_dummy' and 'region_dummy' respectively. <br>The parameters used in 'get_dummies' is that the column in which encoding is done and 'dtype=int' indicates that the value stores in the dummy is of 0 and 1 type.<br>\n",
        "//line2 and line5: Concatenating the dummy columns in the original dataset and setting 'axis=1' to concat the dummy values columnwise. <br>\n",
        "//line3 and line6: Drop the 'Region' and 'Cause' columns as it is being distributed in the dummy values.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Qndprbssxaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding Categorical Data using Label Encoding Technique\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "data_set['Status']=le.fit_transform(data_set['Status'])\n",
        "data_set['Sex']=le.fit_transform(data_set['Sex'])\n",
        "data_set.head(5)"
      ],
      "metadata": {
        "id": "3jcX2WXfoumc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Label Encoding Technique to encode categorical data which contains 2 unique values.<br>\n",
        ">line1: Importing LabelEncoder module from Scikit library<br>\n",
        "line2: Creating the instance(object) of LabelEncoder named 'le'<br>\n",
        "line3&4: Applying the LabelEncoder in 'Status' and 'Sex' column with the use of 'fit_transform()' method  "
      ],
      "metadata": {
        "id": "5K1C5hKDve4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividing unto Target and Predictor Variables\n",
        "X=data_set.iloc[:,0:15]#predictor variables\n",
        "y=data_set.iloc[:,15:25]#target variables\n",
        "X"
      ],
      "metadata": {
        "id": "mNPKK_gDbu7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, next step in Data Preprocessing is to divide the dataset into Target variable and Predictor Variable.<br>\n",
        "\n",
        "\n",
        "*   Target Variable: <br>\n",
        "The variable(s) in the dataset which predict the outcome of the machine learning model is known as Target Variable. <br>It is also known as 'dependent variable' as it depends on various feature(column) to predict the outcome.\n",
        "*   Predictor Variable: <br>\n",
        "The variable(s) whose values are used to predict the target variable is known as Predictor Variable. <br>It is also known as 'independent variable' as with the help of these variables the target variable is defined.<br>\n",
        "Target Variable -> \"Region\" and Predictor Variable -> \"Status, Sex, Cause\"\n",
        "\n",
        "line1: 'X' is a symbol Convention used for Predictor variable. Used 'iloc' to access rows and columns and list is taken to access rows and columns.\n",
        "\n",
        "*   1st parameter is for rows ':' -> all rows (no range is defined)\n",
        "*   2nd parameter is for columns '0:12' -> includes Status, Sex, and Cause (which were divided during encoding)\n",
        "\n",
        "line2: Similarly, 'y' is a sign convention used for Target Variable and the rest are same as line1 but the difference is that 2nd parameter is '12:22' includes all the values of Region which were divided during encoding.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dct0Lhu3aFPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividing Data as Training and Testing Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "#creating instance of train_test_split of Predictor and Target variables respectively\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=78)\n",
        "X_test.head(5)"
      ],
      "metadata": {
        "id": "KaPhjwv6WAwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.head(5)"
      ],
      "metadata": {
        "id": "qc8NwQLXh8bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, next step is to divide the dataset into Training Set and Test Set using Scikit Library<br>\n",
        "line1: import 'train_test_split' function from sklearn.model_selection<br>\n",
        "Creating instance of \"train_test_split\" for Predictor and Target variables respectively\n",
        "\n",
        "*   X_train, X_test: used to split the Predictor Variables into Train Set and Test Set respectively\n",
        "*   y_train, y_test: used to split the Target Variables into Train Set and Test Set respectively\n",
        "\n",
        "\" X_train,X_test,y_train,y_test \" -> must be writteen in this format as 'train_test_split()' function divides this way only <br>\n",
        "Parameters of the function includes X,y as the function works for it<br> \"test_size\" is specified so that train and test set gets split into 8:2 ratio<br>\n",
        "\"random_state\" is specified to randomly select the data points from the dataset which will be same for both Predictor and target Variable (i.e, X and y)\n",
        "\n"
      ],
      "metadata": {
        "id": "gVBM6hNUjAg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Scaling on the Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "X_train.iloc[:,3:5]=sc.fit_transform(X_train.iloc[:,3:5])\n",
        "X_test.iloc[:,3:5]=sc.transform(X_test.iloc[:,3:5])\n",
        "X_train"
      ],
      "metadata": {
        "id": "Jjcgoh0eyBiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will perform Feature Scaling in the dataset to normalize the varying range of the independent feature(column). It is perform in order to interpret these features on the same scale in Machine Learning.<br>\n",
        "line1: importing StandardScaler from sklearn.preprocessing which is used to create a same scale for the entire column(i.e., Rate and SE column)<br>\n",
        "line2: Whenever we import a function we must create an instance to access it and named it \"sc\"<br>\n",
        "line3: For feature scaling, we calculate the mean and standard deviation of a coulmn so for that we use \"fit_transform()\". \"fit\" is used to calculate the\n",
        "\n",
        "*   \"fit\" is used to calculate the mean and standard deviation for the column\n",
        "*   \"transform\" will use the obtained calculation and apply it to the rest of the data points of the column<br>\n",
        "\n",
        "\"fit_transform()\" is applied to \"Rate and SE\" columns so \"iloc\" is used where \" [ : , 3 : 5] \" where 1st parameter(i.e., : , ) will take all rows of \"X_train\" and 2nd parameter(i.e., 3:5) will take only \"Rate and SE\" column so its index value is mentioned <br>\n",
        "line4: Similarly, the same thing is being performed for \"X_test\" dataset but the changes is that only transform function is used as we don't want to know the mean and standard deviation of the test dataset which makes the dataset to maintain the accuracy which is obtained by train dataset."
      ],
      "metadata": {
        "id": "kQAcg-Ht8gLg"
      }
    }
  ]
}