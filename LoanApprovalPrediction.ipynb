{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWbJQkYbCevhIdR+VUx3wK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaliGit-1/Data_Science_Colab/blob/main/LoanApprovalPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ilbyEV5_MJcb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "dataset_l=pd.read_csv(\"Training Dataset.csv\")\n",
        "dataset_l.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "->Importing all the necessary libraries such as Numpy and Pandas<br>\n",
        "->Read the dataset of csv file in order to work on it and save it to the dataframe named \"dataset_l\"  "
      ],
      "metadata": {
        "id": "-czruJSika3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_l.isnull().sum()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oUOmf7zBQlPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will display how many rows in a column have null values in it<br>\n",
        "-> isnull(): check each row of a column whether it is null or not<br>\n",
        "-> sum(): count how many rows in a column is null"
      ],
      "metadata": {
        "id": "gx6A9ui_kxHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Handling Missing Values of Numeric Columns(i.e., LoanAmount,Loan_Amount_Term,Credit_History)\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer=SimpleImputer(missing_values=np.NaN,strategy=\"mean\")\n",
        "dataset_l.iloc[:,8:11]=imputer.fit_transform(dataset_l.iloc[:,8:11])\n",
        "#Handling Missing Values of the object columns(i.e.,Gender,Married,Dependents,Self_Employed)\n",
        "dataset_l=dataset_l.dropna()\n",
        "dataset_l"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mFPL60UqWW2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing SimpleImputer class from Sci-Kit learn Library<br>\n",
        "-> SimpleImputer() method: convert the missing values (i.e., NaN) into the defined strategy (i.e., mean) Conversion is fone into numeric columns only<br>\n",
        "-> fit_transform() method: It is the combination of both fit() method which helps to compute the mean of a column and transform() method is used to apply that mean to its respective column. So, instead to write the two functions seperately it fit_transform() method is being called<br>\n",
        "-> dropna() method: Delete all the rows which contains NULL values"
      ],
      "metadata": {
        "id": "aD1_tqoWlOOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode Categorical Variables\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "dataset_l['Gender']=le.fit_transform(dataset_l['Gender'])\n",
        "dataset_l['Married']=le.fit_transform(dataset_l['Married'])\n",
        "dataset_l['Education']=le.fit_transform(dataset_l['Education'])\n",
        "dataset_l['Self_Employed']=le.fit_transform(dataset_l['Self_Employed'])\n",
        "dataset_l['Loan_Status']=le.fit_transform(dataset_l['Loan_Status'])\n",
        "dataset_l"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5hf1Q71sziYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing LabelEncoder class from Sci-Kit learn library<br>\n",
        "-> Creating an instance (i.e., object) of LabelEncoder class and named it \"le\"<br>\n",
        "-> Here, all the Categorical Values such as \"Gender\", \"Married\", \"Education\", \"Self_Employed\" and \"Loan_Status\" are being converted into numeric values (i.e., 0 & 1) because ML model only understand numeric value<br>\n",
        "-> display the dataset_l"
      ],
      "metadata": {
        "id": "S-9TplxqmzMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding the 'Dependents' Categorical Variables\n",
        "dataset_l['Dependents']=pd.to_numeric(dataset_l['Dependents'],errors='coerce').fillna(4).astype(int)\n",
        "dataset_l['Dependents']"
      ],
      "metadata": {
        "id": "87AohjXK2Tv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> to_numeric() method: Convert the column into numeric value and errors='coerce' means that all the non-numeric values replce into NaN<br>\n",
        "-> fillna(4) method: fill the NULL values places to the defined number as passed in the parenthesis (i.e., 4) <br>\n",
        "-> astype(int) method: change the dataype into that datatype which is passed through the parenthesis (i.e., int)<br>\n",
        "-> display the 'Dependent' column"
      ],
      "metadata": {
        "id": "KDyWntEKjo2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding the 'Property_Area' Categorical Variables\n",
        "propertyarea_dummy=pd.get_dummies(dataset_l['Property_Area'],dtype=int)\n",
        "dataset_l=pd.concat([dataset_l,propertyarea_dummy],axis=1)\n",
        "dataset_l.drop('Property_Area',axis=1,inplace=True)\n",
        "dataset_l"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F1EN-3vVjQd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> get_dummies() method: create the different columns for each category value present in a column and return 1 to a certain column and rest 0 for a particular value in a row<br>\n",
        "-> dtype property is to return int type value<br>\n",
        "-> concat() method: add those additional columns in the main dataset and axis=1 refer to columnwise concatenation<br>\n",
        "-> drop() method: delete the column 'Property_Area' as it is divided into different columns so no need of the column<br>\n",
        "-> display the dataset"
      ],
      "metadata": {
        "id": "zcufFCXjjmCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Handling outliers for 'ApplicantIncome' Column\n",
        "def handling_outliers(dataset):\n",
        "  quantile_25=dataset['ApplicantIncome'].astype(float).quantile(0.25)\n",
        "  quantile_75=dataset['ApplicantIncome'].astype(float).quantile(0.75)\n",
        "  iqr=quantile_75-quantile_25\n",
        "  lower_percentile = quantile_25 - 1.5 * iqr\n",
        "  upper_percentile = quantile_75 + 1.5 * iqr\n",
        "\n",
        "  dataset = dataset[dataset['ApplicantIncome'].astype(float) >= lower_percentile]\n",
        "  dataset = dataset[dataset['ApplicantIncome'].astype(float) <= upper_percentile]\n",
        "\n",
        "  return dataset\n",
        "dataset_l=handling_outliers(dataset_l)\n",
        "dataset_l"
      ],
      "metadata": {
        "id": "u5ZOGFmCm9BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers are data points that differ significantly from the majority of the other data. It is being handled so that dataset can be inside a certain region in order to analyze the ML model.<br>\n",
        "Calculates bounds based on the Inter Quantile Range to identify potential outliers, and filters out any values that fall outside the bounds. This process helps clean the dataset by removing unusually high or low 'ApplicantIncome' values.<br>\n",
        ">-> define a function named 'handling_outliers' where dataset as a parameter is being passed<br>\n",
        "-> quantile_25: The 25th percentile (or 1st quartile) of the 'ApplicantIncome' column<br>\n",
        "-> quantile_75: The 75th percentile (or 3rd quartile) of the 'ApplicantIncome' column<br>\n",
        "->Both values are converted to float type before quantile calculation<br>\n",
        "-> The 'iqr' is the difference between the 75th and 25th percentiles. It represents the middle 50% of the data and is often used for detecting outliers<br>\n",
        "-> The lower bound is calculated as 1.5 times the IQR below the 25th percentile.\n",
        "The upper bound is calculated as 1.5 times the IQR above the 75th percentile.\n",
        "These boundaries are typical for outlier detection<br>\n",
        "-> The dataset is filtered to keep only the rows where 'ApplicantIncome' is within the calculated lower and upper bounds, effectively removing the outliers<br>\n",
        "-> The function returns the dataset after removing the outliers<br>\n",
        "\n",
        "Now, 'dataset_l' is going to use the function 'handling_outliers'"
      ],
      "metadata": {
        "id": "MY861pb9sxPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#taking the dependent variable out\n",
        "y=dataset_l.pop('Loan_Status')\n",
        "y"
      ],
      "metadata": {
        "id": "Wse60SwGoLdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking th edependent (i.e., target) variabel out"
      ],
      "metadata": {
        "id": "5LZG6DGByHjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform into same scale\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "dataset_l.drop('Loan_ID',axis=1,inplace=True)\n",
        "sc=StandardScaler()\n",
        "X=sc.fit_transform(dataset_l)\n",
        "X"
      ],
      "metadata": {
        "id": "a4aidrcMsSjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the StandardScaler class from sklearn library<br>\n",
        "-> deletimg the Loan_ID<br>\n",
        "-> creating an instance of StandardScaler class named 'sc' which will help to standardizes the columns of a dataset so that they have a mean of 0 and a standard deviation of 1<br>\n",
        "-> fit_transform() method will help to apply the StandardScaler to all the columns of a dataset<br>\n",
        "-> display 'X'\n",
        "\n"
      ],
      "metadata": {
        "id": "C-bk8shvzlrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.8,random_state=0)\n",
        "X_train"
      ],
      "metadata": {
        "id": "GJfsE730t6_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> importing the train_test_split class from Sci-Kit learn library<br>\n",
        "-> train_test_split() method is used to split the dataset into training dataset and testing dataset. Here, parameters such as\n",
        "\n",
        "*   'train_size' is used to declare what proportion of dataset is being used as train set (i.e., 80% is used as train set remaing 20% is used for test set)\n",
        "*   'random_state' is used to ensures that the same split occurs every time the code is run\n",
        "\n",
        "-> display the X_train set i.e., train set of independent variable"
      ],
      "metadata": {
        "id": "9wBUIdxg1vZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#display the X_test set i.e., test set of independent variable\n",
        "X_test"
      ],
      "metadata": {
        "id": "93sp5jo-xVU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the y_train set i.e., train set of dependent variable\n",
        "y_train"
      ],
      "metadata": {
        "id": "cG-8M3rKxb4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the y_test set i.e., test set of dependent variable\n",
        "y_test"
      ],
      "metadata": {
        "id": "Hdu0ekLLxcMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#Train the Logistic Regression model\n",
        "model=LogisticRegression()\n",
        "model.fit(X_train,y_train)\n",
        "#Make predictions\n",
        "y_pred=model.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "vhNTVAKzxv1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Import the LogisticRegression class from Sci-Kit learn library of linear_model module<br>\n",
        "-> Logistic Regression is being used to predicts the probability of a binary outcome (0 or 1) based on input features and instance of LogisticRegression is being created named 'model'<br>\n",
        "-> fit() method: train the logistic regression model on the training data<br>\n",
        "-> predict() method: generate predictions based on the test data and save it to 'y_pred'<br>\n",
        "-> display 'y_pred'"
      ],
      "metadata": {
        "id": "_4NUcRvY5-pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate The Model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy=accuracy_score(y_test,y_pred)\n",
        "conf_matrix=confusion_matrix(y_test,y_pred)\n",
        "class_report=classification_report(y_test,y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')"
      ],
      "metadata": {
        "id": "wbeekMivhP8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Import the required class for evaluating the model<br>\n",
        "-> accuracy_score() method is used to check how accurate is the prediction of the ML model <br>\n",
        "-> confusion_matrix() method: evaluate the performance of the model by comapring the predicted class labels with the actual class labels<br>\n",
        "-> classification_report() method: creates a classification summary by displaying the 'Precision', 'Recall', 'F1_score' and 'Support' and provides where areas of improvement need to look."
      ],
      "metadata": {
        "id": "9QhtNGW27S6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set display options to show max rows\n",
        "pd.set_option('display.max_rows',None)\n",
        "#Create the DataFrame with true and predicted labels\n",
        "result_df=pd.DataFrame({'True Label':y_test,'Predicted Label':y_pred})\n",
        "result_df.head(10)"
      ],
      "metadata": {
        "id": "A_REjHvijLd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Here, I have seperated the DataFrame into 'True Labels' and 'Predicted Labels' and named it as 'result_df' and display it"
      ],
      "metadata": {
        "id": "MXm1PSGB9tdP"
      }
    }
  ]
}